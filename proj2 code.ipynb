{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('Submitted By')\n",
    "print ('UBITname      = karanman')\n",
    "print ('Person Number = 50290755')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-01T17:38:21.474655Z",
     "start_time": "2018-11-01T17:38:20.855562Z"
    }
   },
   "outputs": [],
   "source": [
    "# importing packages\n",
    "# A package is a collection/directory of python modules\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "import os\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score, mean_squared_error, accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-01T17:38:21.487256Z",
     "start_time": "2018-11-01T17:38:21.477653Z"
    }
   },
   "outputs": [],
   "source": [
    "# There are 3 solutions for the given problem, one is linear regression solution, second is logistic regression solution and the third is neural network solution\n",
    "# Opening and reading the csv files \n",
    "# taking two image pairs from different and similar pairs csv files conactenating the features\n",
    "# taking two image pairs from different and similar pairs csv files subtracting the features\n",
    "# csv file is a file with comma separated values\n",
    "# code for merging features\n",
    "def merge_features(pairs, feat):\n",
    "    \n",
    "        a = pairs.loc[:,'img_id_A'].values\n",
    "        b = pairs.loc[:,'img_id_B'].values\n",
    "\n",
    "        \n",
    "\n",
    "        a_ = feat.loc[a]\n",
    "        b_ = feat.loc[b]\n",
    "\n",
    "        a_.reset_index(inplace=True)\n",
    "        b_.reset_index(inplace=True)\n",
    "\n",
    "        columns = a_.columns.values\n",
    "\n",
    "        a_new_columns = []\n",
    "        b_new_columns = []\n",
    "\n",
    "        for column in columns:\n",
    "            a_new_col = 'a_' + column\n",
    "            b_new_col = 'b_' + column\n",
    "            a_new_columns.append(a_new_col)\n",
    "            b_new_columns.append(b_new_col)\n",
    "\n",
    "\n",
    "\n",
    "        a_.columns = a_new_columns\n",
    "        b_.columns = b_new_columns\n",
    "\n",
    "        a_b = pd.concat([a_, b_], axis=1)\n",
    "\n",
    "        b_id = a_b.loc[:, 'b_img_id']\n",
    "\n",
    "        a_b.drop('b_img_id', axis=1, inplace=True)\n",
    "\n",
    "        a_b = pd.concat([b_id, a_b], axis=1)\n",
    "\n",
    "        a_id = a_b.loc[:, 'a_img_id']\n",
    "\n",
    "        a_b.drop('a_img_id', axis=1, inplace=True)\n",
    "\n",
    "        a_b = pd.concat([a_id, a_b], axis=1)\n",
    "\n",
    "        return a_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-01T17:38:21.496704Z",
     "start_time": "2018-11-01T17:38:21.490024Z"
    }
   },
   "outputs": [],
   "source": [
    "# subtracting the features of two images\n",
    "def sub_features(pairs, feat):\n",
    "\n",
    "    a = pairs.loc[:,'img_id_A'].values\n",
    "    b = pairs.loc[:,'img_id_B'].values\n",
    "\n",
    "    a_ = feat.loc[a]\n",
    "    b_ = feat.loc[b]\n",
    "\n",
    "    sub_df = a_.reset_index(drop=True).subtract(b_.reset_index(drop=True))\n",
    "\n",
    "    sub_df['a_img_id'] = a_.index.values\n",
    "\n",
    "    sub_df['b_img_id'] = b_.index.values\n",
    "\n",
    "    sub_df = sub_df.reindex_axis(sorted(sub_df.columns), axis=1)\n",
    "\n",
    "    return sub_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-01T17:38:21.510262Z",
     "start_time": "2018-11-01T17:38:21.499833Z"
    }
   },
   "outputs": [],
   "source": [
    "# generating data with concatenation of features by using human observed data set\n",
    "# generating data with subtraction of features by using human observed data set\n",
    "# Opening and reading the csv file \n",
    "def genHumanDataSet():\n",
    "    \n",
    "    path = 'data/HumanObserved-Dataset/HumanObserved-Features-Data/'\n",
    "    feat_path = os.path.join(path, 'HumanObserved-Features-Data.csv')  \n",
    "    \n",
    "    diff_pairs_path = os.path.join(path, 'diffn_pairs.csv')\n",
    "    same_pairs_path = os.path.join(path, 'same_pairs.csv')\n",
    "    \n",
    "    feat = pd.read_csv(feat_path, index_col=0)\n",
    "    diff_pairs = pd.read_csv(diff_pairs_path)\n",
    "    same_pairs = pd.read_csv(same_pairs_path)\n",
    "    \n",
    "    feat.set_index('img_id', inplace=True)    \n",
    "        \n",
    "    same_pairs_df = merge_features(same_pairs, feat)\n",
    "    diff_pairs_df = merge_features(diff_pairs, feat)\n",
    "    \n",
    "    same_pairs_df = pd.concat([same_pairs_df, same_pairs], axis=1)\n",
    "    same_pairs_df.drop(['img_id_A', 'img_id_B'], axis=1, inplace=True)\n",
    "    \n",
    "    diff_pairs_df = pd.concat([diff_pairs_df, diff_pairs], axis=1)\n",
    "    diff_pairs_df.drop(['img_id_A', 'img_id_B'], axis=1, inplace=True)\n",
    "    \n",
    "    human_final_df = pd.concat([same_pairs_df, diff_pairs_df])\n",
    "    \n",
    "    human_final_df.to_csv('human_concat_final_df.csv')  \n",
    "    \n",
    "    same_pairs_df = sub_features(same_pairs, feat)\n",
    "    diff_pairs_df = sub_features(diff_pairs, feat)\n",
    "    \n",
    "    same_pairs_df = pd.concat([same_pairs_df, same_pairs], axis=1)\n",
    "    same_pairs_df.drop(['img_id_A', 'img_id_B'], axis=1, inplace=True)\n",
    "    \n",
    "    diff_pairs_df = pd.concat([diff_pairs_df, diff_pairs], axis=1)\n",
    "    diff_pairs_df.drop(['img_id_A', 'img_id_B'], axis=1, inplace=True)\n",
    "    \n",
    "    human_final_df = pd.concat([same_pairs_df, diff_pairs_df])\n",
    "    \n",
    "    human_final_df.to_csv('human_subtract_final_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-01T17:38:21.522971Z",
     "start_time": "2018-11-01T17:38:21.513398Z"
    }
   },
   "outputs": [],
   "source": [
    "# generating data with concatenation of features by using GSC features data set\n",
    "# generating data with subtraction of features by using GSC features data set\n",
    "# Opening and reading the csv file \n",
    "def genGSCDataSet():\n",
    "    path = 'data/GSC-Dataset/GSC-Features-Data/'\n",
    "    feat_path = os.path.join(path, 'GSC-Features.csv')\n",
    "    \n",
    "    diff_pairs_path = os.path.join(path, 'diffn_pairs.csv')\n",
    "    same_pairs_path = os.path.join(path, 'same_pairs.csv')\n",
    "    \n",
    "    feat = pd.read_csv(feat_path, index_col=0)\n",
    "    diff_pairs = pd.read_csv(diff_pairs_path)\n",
    "    same_pairs = pd.read_csv(same_pairs_path)\n",
    "    \n",
    "    same_pairs_df = merge_features(same_pairs, feat)\n",
    "        \n",
    "    same_pairs_df.to_csv('gsc_concat_final_df.csv')\n",
    "    \n",
    "    print(\"same pairs df shape : {}\".format(same_pairs_df.shape))\n",
    "    \n",
    "    # split diff into 55k dfs\n",
    "    start = 0\n",
    "    step = 55000 # depends on available ram\n",
    "    max_ = diff_pairs.shape[0]\n",
    "    \n",
    "    for i in range(1, int(math.floor(diff_pairs.shape[0]/step))+1):\n",
    "        \n",
    "        stop = i * step\n",
    "        \n",
    "        if stop > max_:\n",
    "            stop = max_ + 1\n",
    "        \n",
    "        print('stop : {}'.format(stop))\n",
    "        \n",
    "        diff_pairs_part = diff_pairs.iloc[start:stop]\n",
    "        \n",
    "        start = stop\n",
    "    \n",
    "        diff_pairs_df = merge_features(diff_pairs_part, feat)\n",
    "    \n",
    "        diff_pairs_df = pd.concat([diff_pairs_df, diff_pairs_part], axis=1)\n",
    "        diff_pairs_df.drop(['img_id_A', 'img_id_B'], axis=1, inplace=True)\n",
    "    \n",
    "        print(\"diff pairs df shape : {}\".format(diff_pairs_df.shape))\n",
    "                \n",
    "        with open('gsc_concat_final_df.csv', 'a') as f:\n",
    "            diff_pairs_df.to_csv(f, header=False,index=False)\n",
    "        \n",
    "        del diff_pairs_part\n",
    "        del diff_pairs_df\n",
    "        \n",
    "        \n",
    "    same_pairs_df = sub_features(same_pairs, feat)\n",
    "        \n",
    "    same_pairs_df.to_csv('gsc_subtract_final_df.csv')\n",
    "    \n",
    "    print(\"same pairs df shape : {}\".format(same_pairs_df.shape))\n",
    "    \n",
    "    # split diff into 55k dfs\n",
    "    start = 0\n",
    "    step = 55000 # depends on available ram\n",
    "    max_ = diff_pairs.shape[0]\n",
    "    \n",
    "    for i in range(1, int(math.floor(diff_pairs.shape[0]/step))+1):\n",
    "        \n",
    "        stop = i * step\n",
    "        \n",
    "        if stop > max_:\n",
    "            stop = max_ + 1\n",
    "        \n",
    "        print('stop : {}'.format(stop))\n",
    "        \n",
    "        diff_pairs_part = diff_pairs.iloc[start:stop]\n",
    "        \n",
    "        start = stop\n",
    "    \n",
    "        diff_pairs_df = merge_features(diff_pairs_part, feat)\n",
    "    \n",
    "        diff_pairs_df = pd.concat([diff_pairs_df, diff_pairs_part], axis=1)\n",
    "        diff_pairs_df.drop(['img_id_A', 'img_id_B'], axis=1, inplace=True)\n",
    "    \n",
    "        print(\"diff pairs df shape : {}\".format(diff_pairs_df.shape))\n",
    "                \n",
    "        with open('gsc_subtract_final_df.csv', 'a') as f:\n",
    "            diff_pairs_df.to_csv(f, header=False,index=False)\n",
    "        \n",
    "        del diff_pairs_part\n",
    "        del diff_pairs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-01T17:38:35.137115Z",
     "start_time": "2018-11-01T17:38:21.525871Z"
    }
   },
   "outputs": [],
   "source": [
    "# generating the human-observed and GSC data set final file\n",
    "genHumanDataSet()\n",
    "genGSCDataSet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-01T17:38:35.145754Z",
     "start_time": "2018-11-01T17:38:35.142166Z"
    }
   },
   "outputs": [],
   "source": [
    "chunksize = 55000\n",
    "for chunk in pd.read_csv('gsc_concat_final_df.csv', chunksize=chunksize, engine='python'):\n",
    "    print(\"shape: {}\".format(chunk.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-01T17:38:35.165872Z",
     "start_time": "2018-11-01T17:38:35.148757Z"
    }
   },
   "outputs": [],
   "source": [
    "# linear regression solution\n",
    "def linear_regression(dataset='human', mode='concat'):\n",
    "    \n",
    "    data = pd.read_csv('{}_{}_final_df.csv'.format(dataset, mode), index_col=[1,2])\n",
    "    data.drop('Unnamed: 0', axis=1, inplace=True)\n",
    "\n",
    "    X = data.drop('target', axis=1).as_matrix()\n",
    "    y = data['target']\n",
    "    X_train, X_valtest, y_train, y_valtest = train_test_split(X, y, test_size=0.3)\n",
    "    X_val, X_test, y_val, y_test = train_test_split(X_valtest, y_valtest, test_size=0.33)\n",
    "\n",
    "    y = data['target'].values\n",
    "\n",
    "    X_train, X_valtest, y_train, y_valtest = train_test_split(X, y, test_size=0.3)\n",
    "\n",
    "    X_val, X_test, y_val, y_test = train_test_split(X_valtest, y_valtest, test_size=0.33)\n",
    "    \n",
    "    \n",
    "    class LinearRegression(object):\n",
    "        def __init__(self, lr=0.1, n_iter=50):\n",
    "            self.lr = lr\n",
    "            self.n_iter = n_iter\n",
    "\n",
    "        def fit(self, X, y):\n",
    "            X = np.insert(X, 0, 1, axis=1)\n",
    "            self.w = np.ones(X.shape[1])\n",
    "            m = X.shape[0]\n",
    "\n",
    "            for _ in range(self.n_iter):\n",
    "                output = X.dot(self.w)\n",
    "                errors = y - output\n",
    "                self.w += self.lr / m * errors.dot(X)\n",
    "            return self\n",
    "\n",
    "        def predict(self, X):\n",
    "            return np.insert(X, 0, 1, axis=1).dot(self.w)\n",
    "\n",
    "        def score(self, X, y):\n",
    "            return 1 - sum((self.predict(X) - y)**2) / sum((y - np.mean(y))**2)\n",
    "    \n",
    "    # tuning the hyperparameters with these values\n",
    "    if dataset == 'human':\n",
    "        lrs = [0.01, 0.001]\n",
    "        n_iters = [5, 20, 50, 100, 250]\n",
    "    if dataset == 'gsc':\n",
    "        lrs = [0.01, 0.001]\n",
    "        n_iters = [5, 20, 50, 100, 250]\n",
    "\n",
    "\n",
    "    def train_linr_model(lr, n_iter, mode='test'):\n",
    "\n",
    "\n",
    "        print('\\n--Hyperparameters--')\n",
    "        print('learning rate : {}'.format(lr))\n",
    "        print('no of iterations : {}'.format(n_iter))\n",
    "\n",
    "        model = LinearRegression(lr=lr, n_iter=n_iter)\n",
    "\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        if mode == 'val':\n",
    "\n",
    "            y_preds = model.predict(X_val)\n",
    "\n",
    "            rmse = mean_squared_error(y_val, y_preds) ** 0.5\n",
    "\n",
    "            y_preds = np.round(y_preds)\n",
    "\n",
    "            acc = accuracy_score(y_val, y_preds)\n",
    "            \n",
    "            # printing accuracy and Root mean square error for each hyperparameter tuning\n",
    "            print('rmse : {}'.format(rmse))\n",
    "            print('acc : {}'.format(acc))\n",
    "\n",
    "        if mode == 'test':\n",
    "\n",
    "            y_preds = model.predict(X_test)\n",
    "\n",
    "            rmse = mean_squared_error(y_test, y_preds) ** 0.5\n",
    "\n",
    "            y_preds = np.round(y_preds)\n",
    "\n",
    "            acc = accuracy_score(y_test, y_preds)\n",
    "\n",
    "        return acc, rmse\n",
    "    \n",
    "    \n",
    "    scores_df = pd.DataFrame()\n",
    "    for lr in lrs:\n",
    "        for n_iter in n_iters:\n",
    "            acc, rmse = train_linr_model(lr, n_iter, mode='val')\n",
    "            scores = pd.DataFrame([acc, rmse], index=['acc', 'rmse'], columns=[(lr, n_iter)]).T\n",
    "            scores_df = pd.concat([scores_df, scores])\n",
    "    \n",
    "    best_params = scores_df['acc'].idxmax()\n",
    "    lr = best_params[0]\n",
    "    n_iter = best_params[1]\n",
    "    \n",
    "    print('\\n--Training with best hyper-parameters--\\n')\n",
    "\n",
    "    acc, rmse = train_linr_model(lr, n_iter)\n",
    "\n",
    "    print('--Scores on test set--')\n",
    "    print('accuracy : {}'.format(acc))\n",
    "    print('RMSE : {}'.format(rmse))\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-01T17:38:55.424882Z",
     "start_time": "2018-11-01T17:38:35.168656Z"
    }
   },
   "outputs": [],
   "source": [
    "# performing linear regression solution on Human Observed/ concat data set\n",
    "linear_regression(dataset='human', mode='concat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-01T17:39:06.816161Z",
     "start_time": "2018-11-01T17:38:55.430373Z"
    }
   },
   "outputs": [],
   "source": [
    "# performing linear regression solution on Human Observed/ subtract data set\n",
    "linear_regression(dataset='human', mode='subtract')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run The code below for tuning hyperparameters for the GSC data set\n",
    "# this code is added as a comment because it takes a lot of time to tune the hyperparameters for GSC data set\n",
    "# linear_regression(dataset='gsc', mode='concat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run The code below for tuning hyperparameters for the GSC data set\n",
    "# this code is added as a comment because it takes a lot of time to tune the hyperparameters for GSC data set\n",
    "# linear_regression(dataset='gsc', mode='subtract')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-01T17:39:06.858518Z",
     "start_time": "2018-11-01T17:39:06.828210Z"
    }
   },
   "outputs": [],
   "source": [
    "def logistic_regression(dataset='human', mode='concat'):\n",
    "    \n",
    "    data = pd.read_csv('{}_{}_final_df.csv'.format(dataset, mode), index_col=[1,2])\n",
    "    data.drop('Unnamed: 0', axis=1, inplace=True)\n",
    "\n",
    "    X = data.drop('target', axis=1).as_matrix()\n",
    "    y = data['target'].values\n",
    "    \n",
    "    # splitting the data set\n",
    "    X_train, X_valtest, y_train, y_valtest = train_test_split(X, y, test_size=0.3)\n",
    "    X_val, X_test, y_val, y_test = train_test_split(X_valtest, y_valtest, test_size=0.33)\n",
    "    \n",
    "    class LogisticRegression(object):\n",
    "        def __init__(self, lr, num_iter=100, fit_intercept=True, verbose=False):\n",
    "            self.lr = lr\n",
    "            self.num_iter = num_iter\n",
    "            self.fit_intercept = fit_intercept\n",
    "            self.verbose = verbose\n",
    "\n",
    "        def __add_intercept(self, X):\n",
    "            intercept = np.ones((X.shape[0], 1))\n",
    "            return np.concatenate((intercept, X), axis=1)\n",
    "\n",
    "        def __sigmoid(self, z):\n",
    "            return 1 / (1 + np.exp(-z))\n",
    "        def __loss(self, h, y):\n",
    "            return (-y * np.log(h) - (1 - y) * np.log(1 - h)).mean()\n",
    "\n",
    "        def fit(self, X, y):\n",
    "            if self.fit_intercept:\n",
    "                X = self.__add_intercept(X)\n",
    "\n",
    "            # weights initialization\n",
    "            self.theta = np.zeros(X.shape[1])\n",
    "\n",
    "            for i in range(self.num_iter):\n",
    "                z = np.dot(X, self.theta)\n",
    "                h = self.__sigmoid(z)\n",
    "                gradient = np.dot(X.T, (h - y)) / y.size\n",
    "                self.theta -= self.lr * gradient\n",
    "\n",
    "                if(self.verbose == True and i % 10000 == 0):\n",
    "                    z = np.dot(X, self.theta)\n",
    "                    h = self.__sigmoid(z)\n",
    "                    print(f'loss: {self.__loss(h, y)} \\t')\n",
    "\n",
    "        def predict_prob(self, X):\n",
    "            if self.fit_intercept:\n",
    "                X = self.__add_intercept(X)\n",
    "\n",
    "            return self.__sigmoid(np.dot(X, self.theta))\n",
    "\n",
    "        def predict(self, X, threshold=0.5):\n",
    "            return self.predict_prob(X) >= threshold\n",
    "\n",
    "    if dataset == 'human':\n",
    "        lrs = [0.01, 0.001]\n",
    "        n_iters = [5, 20, 50, 100, 250]\n",
    "        \n",
    "    if dataset == 'gsc':\n",
    "        lrs = [0.01, 0.001]\n",
    "        n_iters = [5, 20, 50, 100, 250]\n",
    "\n",
    "\n",
    "\n",
    "    def train_logr_model(lr, n_iter, mode='test'):\n",
    "\n",
    "\n",
    "        print('\\n--Hyperparameters--')\n",
    "        print('learning rate : {}'.format(lr))\n",
    "        print('no of iterations : {}'.format(n_iter))\n",
    "\n",
    "        model = LogisticRegression(lr=lr, num_iter=n_iter)\n",
    "\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        if mode == 'val':\n",
    "\n",
    "            y_preds = model.predict(X_val)\n",
    "\n",
    "            rmse = mean_squared_error(y_val, y_preds) ** 0.5\n",
    "\n",
    "            acc = accuracy_score(y_val, y_preds)\n",
    "\n",
    "            print('rmse : {}'.format(rmse))\n",
    "            print('acc : {}'.format(acc))\n",
    "\n",
    "        if mode == 'test':\n",
    "\n",
    "            y_preds = model.predict(X_test)\n",
    "\n",
    "            rmse = mean_squared_error(y_test, y_preds) ** 0.5\n",
    "\n",
    "            acc = accuracy_score(y_test, y_preds)\n",
    "\n",
    "        return acc, rmse\n",
    "    \n",
    "    \n",
    "    scores_df = pd.DataFrame()\n",
    "    for lr in lrs:\n",
    "        for n_iter in n_iters:\n",
    "            acc, rmse = train_logr_model(lr, n_iter, mode='val')\n",
    "            scores = pd.DataFrame([acc, rmse], index=['acc', 'rmse'], columns=[(lr, n_iter)]).T\n",
    "            scores_df = pd.concat([scores_df, scores])\n",
    "    \n",
    "    best_params = scores_df['acc'].idxmax()\n",
    "    lr = best_params[0]\n",
    "    n_iter = best_params[1]\n",
    "    \n",
    "    print('\\n--Training with best hyper-parameters--\\n')\n",
    "\n",
    "    acc, rmse = train_logr_model(lr, n_iter, mode='test')\n",
    "\n",
    "    print('--Scores on test set--')\n",
    "    print('accuracy : {}'.format(acc))\n",
    "    print('RMSE : {}'.format(rmse))\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-01T17:39:13.022483Z",
     "start_time": "2018-11-01T17:39:06.862584Z"
    }
   },
   "outputs": [],
   "source": [
    "# performing logistic regression solution on Human Observed/ concat data set\n",
    "logistic_regression(dataset='human', mode='concat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-01T17:39:18.016755Z",
     "start_time": "2018-11-01T17:39:13.028103Z"
    }
   },
   "outputs": [],
   "source": [
    "# performing logistic regression solution on Human Observed/ subtract data set\n",
    "logistic_regression(mode='subtract')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run The code below for tuning hyperparameters for the GSC data set\n",
    "# this code is added as a comment because it takes a lot of time to tune the hyperparameters for GSC data set\n",
    "# logistic_regression(dataset='gsc', mode='concat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run The code below for tuning hyperparameters for the GSC data set\n",
    "# this code is added as a comment because it takes a lot of time to tune the hyperparameters for GSC data set\n",
    "# logistic_regression(dataset='gsc', mode='subtract')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-01T17:39:18.228510Z",
     "start_time": "2018-11-01T17:39:18.024815Z"
    }
   },
   "outputs": [],
   "source": [
    "# importing packages\n",
    "# A package is a collection/directory of python modules\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-01T17:39:18.750292Z",
     "start_time": "2018-11-01T17:39:18.231175Z"
    }
   },
   "outputs": [],
   "source": [
    "# reading the final created human observed file\n",
    "data = pd.read_csv('human_final_df.csv', index_col=[1,2], engine='c')\n",
    "data.drop('Unnamed: 0', axis=1, inplace=True)\n",
    "y = data['target'].values\n",
    "\n",
    "# splitting the file\n",
    "X_train, X_valtest, y_train, y_valtest = train_test_split(data, y, test_size=0.3)\n",
    "\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_valtest, y_valtest, test_size=0.33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-01T17:39:23.079523Z",
     "start_time": "2018-11-01T17:39:18.753541Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train.to_csv('train_data.csv')\n",
    "X_val.to_csv('val_data.csv')\n",
    "X_test.to_csv('test_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-01T17:39:23.094197Z",
     "start_time": "2018-11-01T17:39:23.087127Z"
    }
   },
   "outputs": [],
   "source": [
    "class CustomDatasetFromCSV(Dataset):\n",
    "    def __init__(self, csv_path):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_path (string): path to csv file\n",
    "        \"\"\"\n",
    "        self.data = pd.read_csv(csv_path)\n",
    "        self.labels = np.asarray(self.data['target'])\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        single_label = self.labels[index]\n",
    "        features = np.asarray(self.data.iloc[index, :-1])\n",
    "        \n",
    "        features_as_tensor = torch.tensor(features)\n",
    "        return (features_as_tensor, single_label)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-01T17:39:23.112745Z",
     "start_time": "2018-11-01T17:39:23.101232Z"
    }
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(28 * 28, 200)\n",
    "        self.fc2 = nn.Linear(200, 200)\n",
    "        self.fc3 = nn.Linear(200, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return F.log_softmax(x)\n",
    "    \n",
    "net = Net()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-01T17:39:23.118484Z",
     "start_time": "2018-11-01T17:39:23.115075Z"
    }
   },
   "outputs": [],
   "source": [
    "# create a stochastic gradient descent optimizer\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=0.01, momentum=0.9)\n",
    "# create a loss function\n",
    "criterion = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-01T17:39:23.532384Z",
     "start_time": "2018-11-01T17:39:23.120979Z"
    }
   },
   "outputs": [],
   "source": [
    "datasetTrain = CustomDatasetFromCSV('train_data.csv')\n",
    "datasetVal = CustomDatasetFromCSV('val_data.csv')\n",
    "datasetTest = CustomDatasetFromCSV('test_data.csv')\n",
    "\n",
    "trBatchSize = 16\n",
    "epochs = 10\n",
    "\n",
    "dataLoaderTrain = DataLoader(dataset=datasetTrain, batch_size=trBatchSize, shuffle=True,  num_workers=24, pin_memory=True)\n",
    "dataLoaderVal = DataLoader(dataset=datasetVal, batch_size=trBatchSize, shuffle=False, num_workers=24, pin_memory=True)\n",
    "dataLoaderTest = DataLoader(dataset=datasetTest, batch_size=trBatchSize, shuffle=False, num_workers=24, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-01T17:39:24.106310Z",
     "start_time": "2018-11-01T17:39:23.534669Z"
    }
   },
   "outputs": [],
   "source": [
    "# run the main training loop\n",
    "for epoch in range(epochs):\n",
    "    for batch_idx, (data, target) in enumerate(dataLoaderTrain):\n",
    "        data, target = Variable(data), Variable(target)\n",
    "        optimizer.zero_grad()\n",
    "        net_out = net(data)\n",
    "        loss = criterion(net_out, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                    epoch, batch_idx * len(data), len(dataLoaderTrain.dataset),\n",
    "                           100. * batch_idx / len(dataLoaderTrain), loss.data[0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
